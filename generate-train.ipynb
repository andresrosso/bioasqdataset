{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import sys,os,os.path\n",
    "import re\n",
    "import logging\n",
    "import logging.config\n",
    "import spacy\n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "\n",
    "logging.config.fileConfig(\"logging.conf\")\n",
    "\n",
    "os.environ['HTTP_PROXY']=\"http://sacorchuelop:escorpion@proxyapp.unal.edu.co:8080/\"\n",
    "os.environ['HTTPS_PROXY']=\"http://sacorchuelop:escorpion@proxyapp.unal.edu.co:8080/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_docs = []\n",
    "\n",
    "def read_pubmed_abstract(url):\n",
    "    r  = requests.get(url)\n",
    "    data = r.text\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "    mydivs = soup.findAll(\"div\", {\"class\": \"abstr\"})\n",
    "    if len(mydivs) > 0:\n",
    "        abstract = mydivs[0].find('p').text\n",
    "        return abstract\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-->Sentence 0: The identification of common variants that contribute to the genesis of human inherited disorders remains a significant challenge.\n",
      "-->Sentence 1: Hirschsprung disease (HSCR) is a multifactorial, non-mendelian disorder in which rare high-penetrance coding sequence mutations in the receptor tyrosine kinase\n",
      "-->Sentence 2: RET contribute to risk in combination with mutations at other genes.\n",
      "-->Sentence 3: We have used family-based association studies to identify a disease interval, and integrated this with comparative and functional genomic analysis to prioritize conserved and functional elements within which mutations can be sought.\n",
      "-->Sentence 4: We now show that a common non-coding RET variant within a conserved enhancer-like sequence in intron 1 is significantly associated with HSCR susceptibility and makes a 20-fold greater contribution to risk than rare alleles do.\n",
      "-->Sentence 5: This mutation reduces in vitro enhancer activity markedly, has low penetrance, has different genetic effects in males and females, and explains several features of the complex inheritance pattern of HSCR.\n",
      "-->Sentence 6: Thus, common low-penetrance variants, identified by association studies, can underlie both common and rare diseases.\n"
     ]
    }
   ],
   "source": [
    "#spacy sentence tokenizer\n",
    "abstract = read_pubmed_abstract(\"http://www.ncbi.nlm.nih.gov/pubmed/15829955\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "abstract = nlp(abstract)\n",
    "for i, token in enumerate(abstract.sents):\n",
    "    print('-->Sentence %d: %s' % (i, token.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate random documents from pubmed ids since 2005 id https://www.ncbi.nlm.nih.gov/pubmed/15829959\n",
    "random_docs = []\n",
    "max_random_docs = 900000\n",
    "for x in range(max_random_docs):\n",
    "    random_docs.append('https://www.ncbi.nlm.nih.gov/pubmed/'+str(random.randint(15829959,28829959)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_files = ['BioASQ-trainingDataset4b.json','BioASQ-trainingDataset5b.json',\n",
    "           'BioASQ-trainingDataset6b.json',\n",
    "           'BioASQ-trainingDataset7b.json',\n",
    "           'BioASQ-trainingDataset8b.json']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Gold Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No snippets for 51406e6223fec90375000009\n",
      "No snippets for 54d643023706e89528000007\n",
      "No snippets for 51593dc8d24251bc05000099\n",
      "No snippets for 532819afd6d3ac6a3400000f\n",
      "No snippets for 5158a5b8d24251bc05000097\n",
      "No snippets for 5172f8118ed59a060a000019\n",
      "No snippets for 517545168ed59a060a00002b\n",
      "Gold snippets size 173253\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Extract the information for training dataset\n",
    "Query\n",
    "body, documents, ideal_answer, concepts, type, id, snippets\n",
    "Snippet\n",
    "offsetInBeginSection, offsetInEndSection, text, beginSection, document, endSection\n",
    "\"\"\"\n",
    "gold_snippets = []\n",
    "gold_docs = []\n",
    "for dataset in dataset_files:\n",
    "    logging.debug(\"BioASQ Generating train files for {}\".format(dataset))\n",
    "    data = json.load(open('train-data/'+dataset,'r'))\n",
    "    for query in data['questions']:\n",
    "        if 'snippets' not in query.keys():\n",
    "            print('No snippets for {}'.format(query['id']))\n",
    "        else:\n",
    "            for snippet in query['snippets']:\n",
    "                snippet['body'] = query['body']\n",
    "                snippet['id'] = query['id']\n",
    "                snippet['trainset'] = dataset\n",
    "                snippet['label'] = 1\n",
    "                snippet['doc_related'] = 1\n",
    "                gold_snippets.append(snippet)\n",
    "        gold_docs.extend(query['documents'])\n",
    "\n",
    "print(\"Gold snippets size {}\".format(len(gold_snippets)))\n",
    "json.dump(gold_snippets,open('train-data/train_pairs/gold_pairs.json','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Related Document But Negative Snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 289/1307 [00:38<01:03, 16.11it/s]"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"common\")\n",
    "import bioasq_util as bioasq_util\n",
    "import ranking\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import importlib\n",
    "importlib.reload(ranking)\n",
    "stops = stopwords.words('english')\n",
    "\n",
    "index_name = '2018_pubmed_baseline_title_abs_mesh'\n",
    "doc_relative_url = 'http://www.ncbi.nlm.nih.gov/pubmed/'\n",
    "\n",
    "logging.getLogger('elasticsearch').setLevel(logging.ERROR)\n",
    "\"\"\"\n",
    "Extract the information for training dataset\n",
    "Query\n",
    "body, documents, ideal_answer, concepts, type, id, snippets\n",
    "Snippet\n",
    "offsetInBeginSection, offsetInEndSection, text, beginSection, document, endSection\n",
    "\"\"\"\n",
    "for dataset in dataset_files:\n",
    "    related_docs_snippets = []\n",
    "    logging.info(\"BioASQ Generating train files for {}\".format(dataset))\n",
    "    data = json.load(open('train-data/'+dataset,'r'))\n",
    "    for query in tqdm.tqdm(data['questions'],position=0):\n",
    "        if 'snippets' in query:\n",
    "            for doc in query['documents']:\n",
    "                if bioasq_util.get_doc(doc.replace(doc_relative_url,''), index_name, remove_tags=True) != None:\n",
    "                    doc_id, doc_title, doc_abstract = bioasq_util.get_doc(doc.replace(doc_relative_url,''), index_name, remove_tags=True)\n",
    "                    if (doc_title != None) & (doc_abstract != None):\n",
    "                        text = doc_title + \" \" +  doc_abstract\n",
    "                        #remove gold sentences\n",
    "                        for snippet in query['snippets']:\n",
    "                            if doc_id in snippet['document']:\n",
    "                                text = text.replace(snippet['text'],\" \")\n",
    "                        #tokenize in snippets\n",
    "                        text_chunks = ranking.split_chunks(text)\n",
    "                        for chunk in text_chunks:\n",
    "                            chunk['body'] = query['body']\n",
    "                            chunk['id'] = query['id']\n",
    "                            chunk['trainset'] = dataset\n",
    "                            chunk['label'] = 0\n",
    "                            chunk['doc_related'] = 1\n",
    "                            related_docs_snippets.append(chunk)\n",
    "        if len(related_docs_snippets) > 540000:\n",
    "            break\n",
    "    print(\"Gold snippets size {}\".format(len(related_docs_snippets)))\n",
    "    json.dump(related_docs_snippets,open('train-data/train_pairs/related_docs_negative_pairs_'+dataset,'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold snippets size 240004\n"
     ]
    }
   ],
   "source": [
    "print(\"Gold snippets size {}\".format(len(related_docs_snippets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_gold_len = []\n",
    "all_q = []\n",
    "big_a = 0\n",
    "for x in gold_snippets:\n",
    "    all_q.append(len(x['body'].split(' ')))\n",
    "    if len(x['text'].split(' ')) > 200:\n",
    "        big_a += 1\n",
    "    else:\n",
    "        all_gold_len.append(len(x['text'].split(' ')))\n",
    "sns.distplot(all_gold_len)\n",
    "sns.distplot(all_q)\n",
    "\n",
    "\"\"\"\n",
    "As the lenght of the answers sequence is around 150 terms,\n",
    "we should end with only answers candidates in this boundary\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Negative Dataset No Related Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import bioasq_util\n",
    "\n",
    "bioasq_util.es = Elasticsearch(hosts=['168.176.36.10:9200'])\n",
    "index_name = '2018_pubmed_baseline_title_abs_mesh'\n",
    "doc_relative_url = 'http://www.ncbi.nlm.nih.gov/pubmed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [2:45:28<00:00,  5.04it/s]  \n"
     ]
    }
   ],
   "source": [
    "#add negative snippet\n",
    "negative_docs = list(set(random_docs) - set(gold_docs))\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "random_snippets = []\n",
    "#generate random snippets\n",
    "logging.debug(\"Generate random snippets from not related document colection size {}\".format(len(negative_docs)))\n",
    "for x in tqdm.tqdm(range(50000),position=0):\n",
    "    if x % 10 == 0:\n",
    "        logging.debug(\"Generate random snippet for file {}\".format(x))\n",
    "        json.dump({'random_snippets':random_snippets},open('train-data/train_pairs/random_snippets.json','w'))\n",
    "    doc_idx = random.randint(0,len(negative_docs))\n",
    "    abstract = bioasq_util.get_doc(negative_docs[doc_idx].split('/')[-1], index_name, remove_tags=True) \n",
    "    if abstract is not None:\n",
    "        abstract = nlp(abstract[2])\n",
    "        random_snippet = None\n",
    "        for i, token in enumerate(abstract.sents):\n",
    "            if (len(token.text) > 50) & (len(token.text) < 500):\n",
    "                random_snippet = token.text\n",
    "                random_snippets.append({'document':negative_docs[doc_idx],\n",
    "                                        'snippet':random_snippet})\n",
    "json.dump({'random_snippets':random_snippets},open('train-data/train_pairs/random_snippets.json','w'))\n",
    "#print('-->Sentence %d: %s' % (i, token.text))\n",
    "#token = get_random_snippet(negative_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(random_snippets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_snippets_random_doc = []\n",
    "\n",
    "#add different document but negative snippet\n",
    "for dataset in dataset_files:\n",
    "    logging.debug(\"BioASQ Generating negative_snippets_random_doc train files for {}\".format(dataset))\n",
    "    data = json.load(open('train-data/'+dataset,'r'))\n",
    "    for query in data['questions']:\n",
    "        if 'snippets' not in query.keys():\n",
    "            print('No snippets for {}'.format(query['id']))\n",
    "        else:\n",
    "            for snippet in query['snippets']:\n",
    "                token = get_random_snippet()\n",
    "                snippet['document'] = negative_docs[doc_idx]\n",
    "                snippet['text'] = token.text\n",
    "                snippet['body'] = query['body']\n",
    "                snippet['id'] = query['id']\n",
    "                snippet['trainset'] = dataset\n",
    "                snippet['label'] = 0\n",
    "                snippet['doc_related'] = 0\n",
    "                snippet['offsetInBeginSection'] = None\n",
    "                snippet['offsetInEndSection'] = None\n",
    "                snippet['beginSection'] = None\n",
    "                snippet['endSection'] = None\n",
    "                negative_snippets_random_doc.append(snippet)\n",
    "            logging.debug(\"qid {}, total snippets {}\".format(query['id'],len(negative_snippets_random_doc)))\n",
    "\n",
    "json.dump(negative_snippets_random_doc,open('train-data/train_pairs/negative_snippets_random_doc.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'offsetInBeginSection': None,\n",
       "  'offsetInEndSection': None,\n",
       "  'text': 'Graft arteriosclerosis (GA), the major cause of late cardiac allograft failure, is characterized by a diffuse, concentric arterial intimal hyperplasia composed of infiltrating host T cells, macrophages, and predominantly graft-derived smooth muscle-like cells that proliferate and elaborate extracellular matrix, resulting in luminal obstruction and allograft ischemia.',\n",
       "  'beginSection': None,\n",
       "  'document': 'https://www.ncbi.nlm.nih.gov/pubmed/20520254',\n",
       "  'endSection': None,\n",
       "  'body': 'What symptoms characterize the Muenke syndrome?',\n",
       "  'id': '52bf1d3c03868f1b0600000d',\n",
       "  'trainset': 'BioASQ-trainingDataset4b.json',\n",
       "  'label': 0,\n",
       "  'doc_related': 0},\n",
       " {'offsetInBeginSection': None,\n",
       "  'offsetInEndSection': None,\n",
       "  'text': 'One year ago, a 42-year-old woman underwent aortic root replacement because of a pseudoaneurysm that developed at the site of an anastomosis after ascending aortic replacement for acute aortic dissection.',\n",
       "  'beginSection': None,\n",
       "  'document': 'https://www.ncbi.nlm.nih.gov/pubmed/20520254',\n",
       "  'endSection': None,\n",
       "  'body': 'What is the inheritance pattern of Li–Fraumeni syndrome?',\n",
       "  'id': '52bf208003868f1b06000019',\n",
       "  'trainset': 'BioASQ-trainingDataset4b.json',\n",
       "  'label': 0,\n",
       "  'doc_related': 0},\n",
       " {'offsetInBeginSection': None,\n",
       "  'offsetInEndSection': None,\n",
       "  'text': 'After transcatheter aortic valve implantation (TAVI) in a 75-year-old male, chronic wide left bundle branch block (LBBB) developed.',\n",
       "  'beginSection': None,\n",
       "  'document': 'https://www.ncbi.nlm.nih.gov/pubmed/20520254',\n",
       "  'endSection': None,\n",
       "  'body': 'What are the indications for alteplase?',\n",
       "  'id': '52bf209303868f1b0600001a',\n",
       "  'trainset': 'BioASQ-trainingDataset4b.json',\n",
       "  'label': 0,\n",
       "  'doc_related': 0},\n",
       " {'offsetInBeginSection': None,\n",
       "  'offsetInEndSection': None,\n",
       "  'text': 'Physical anthropology consists of two interdependent types of study: (1) the biological history of man and (2) general biological processes in man (such as mechanisms of evolution and growth).',\n",
       "  'beginSection': None,\n",
       "  'document': 'https://www.ncbi.nlm.nih.gov/pubmed/20520254',\n",
       "  'endSection': None,\n",
       "  'body': 'What is the role of Inn1 in cytokinesis?',\n",
       "  'id': '530cf22aa177c6630c000001',\n",
       "  'trainset': 'BioASQ-trainingDataset4b.json',\n",
       "  'label': 0,\n",
       "  'doc_related': 0}]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_snippets_random_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.ncbi.nlm.nih.gov/pubmed/20520254'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_docs[doc_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(random_docs))\n",
    "negative_docs = list(set(random_docs) - set(gold_docs))\n",
    "print(len(negative_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
